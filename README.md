# MCP Server para DocumentaÃ§Ã£o de Frameworks (React, Vue, Django, etc.)

Este projeto implementa um servidor de contexto (no estilo MCP) usando **Django + DRF + ChromaDB + Sentence Transformers**. Ele permite que sua IA local (ex.: Continue.dev + qwen2.5-coder) consulte a documentaÃ§Ã£o de qualquer framework ou biblioteca atravÃ©s de busca semÃ¢ntica (RAG). Basta clonar a documentaÃ§Ã£o desejada e rodar a ingestÃ£o â€” o sistema Ã© genÃ©rico e funciona com Markdown, MDX e outros formatos de texto.

## âœ¨ Funcionalidades

- IngestÃ£o automÃ¡tica de documentaÃ§Ã£o a partir de repositÃ³rios Git.
- GeraÃ§Ã£o de embeddings com `sentence-transformers/all-MiniLM-L6-v2`.
- Armazenamento vetorial com ChromaDB (persistente).
- API REST simples para busca contextual.
- FÃ¡cil adaptaÃ§Ã£o para React, Vue, Angular, Django, Laravel, etc.
- IntegraÃ§Ã£o direta com o **Continue.dev** no VS Code.

---

## ğŸ§° PrÃ©-requisitos

- **Python 3.9+**
- **Git**
- **VS Code** com a extensÃ£o **Continue** instalada
- (Opcional) **Ollama** para rodar modelos locais (jÃ¡ configurado no seu `config.yaml`)

---

## ğŸ“¦ InstalaÃ§Ã£o do Servidor

1. **Clone este repositÃ³rio** (ou crie seu prÃ³prio projeto a partir dos arquivos fornecidos):
   ```bash
   git clone https://github.com/projetopy/mcp-all-docs-frameworks
   cd mcp-all-docs-frameworks
   ```

2. **Crie e ative um ambiente virtual**:
   ```bash
   python -m venv venv
   source venv/bin/activate      # Linux/macOS
   venv\Scripts\activate          # Windows
   ```

3. **Instale as dependÃªncias** (use o `requirements.txt` abaixo):
   ```bash
   pip install -r requirements.txt
   ```

   **Arquivo `requirements.txt`** (copie e cole):
   ```
   chromadb
   Django
   djangorestframework
   sentence-transformers
   langchain
   langchain-text-splitters
   ```

4. **Execute as migraÃ§Ãµes iniciais do Django**:
   ```bash
   python manage.py migrate
   ```

---

## ğŸ“¥ Ingerindo a DocumentaÃ§Ã£o de um Framework

O processo Ã© sempre o mesmo: clone a documentaÃ§Ã£o desejada e execute o comando de ingestÃ£o apontando para a pasta.

### Exemplo 1: DocumentaÃ§Ã£o do React

```bash
git clone https://github.com/reactjs/react.dev.git
python manage.py ingest_docs --docs-path ./react.dev
```

### Exemplo 2: DocumentaÃ§Ã£o do Vue.js

```bash
git clone https://github.com/vuejs/docs.git vue-docs
python manage.py ingest_docs --docs-path ./vue-docs
```

### Exemplo 3: DocumentaÃ§Ã£o do Django (em reStructuredText)

Para projetos com documentaÃ§Ã£o em `.rst`, vocÃª pode adaptar o script de ingestÃ£o (futuramente). Mas o mesmo processo funciona para arquivos `.md` e `.mdx`.

**ObservaÃ§Ã£o**: A primeira execuÃ§Ã£o baixarÃ¡ o modelo de embeddings (`all-MiniLM-L6-v2`). Isso pode levar alguns minutos, mas Ã© feito apenas uma vez.

---

## ğŸš€ Executando o Servidor

Com a documentaÃ§Ã£o ingerida, inicie o servidor Django:

```bash
python manage.py runserver
```

A API estarÃ¡ disponÃ­vel em:  
**`http://localhost:8000/api/search/`**

---

## ğŸ” Testando a API

VocÃª pode testar com `curl` ou qualquer cliente HTTP:

```bash
curl -X POST http://localhost:8000/api/search/ \
  -H "Content-Type: application/json" \
  -d '{"query": "useState", "k": 3}'
```

A resposta serÃ¡ um JSON contendo os trechos mais relevantes da documentaÃ§Ã£o, com metadados (caminho do arquivo) e pontuaÃ§Ã£o de distÃ¢ncia.

---

## ğŸ¤– IntegraÃ§Ã£o com o Continue.dev no VS Code

Agora vamos configurar o **Continue** para usar essa API como um provedor de contexto.

### 1. Localize o arquivo de configuraÃ§Ã£o do Continue

No VS Code, abra o Continue e clique no Ã­cone de engrenagem (ou acesse o arquivo diretamente em):
- **Linux/macOS**: `~/.continue/config.yaml #ou pela interface em Settings/Configs` 
	
- **Windows**: `%USERPROFILE%\.continue\config.yaml`

### 2. Adicione um `contextProvider` do tipo HTTP

Abra o arquivo `config.yaml` e adicione o seguinte bloco **apÃ³s a lista de modelos** (respeitando a indentaÃ§Ã£o YAML). Seu arquivo deve ficar parecido com este:

```yaml
name: Local Config
version: 1.0.0
schema: v1
models:
  - name: Llama 3.1 8B
    provider: ollama
    model: llama3.1:8b
    roles:
      - chat
      - edit
      - apply
  - name: Qwen2.5-Coder 1.5B
    provider: ollama
    model: qwen2.5-coder:1.5b-base
    roles:
      - autocomplete
  - name: Nomic Embed
    provider: ollama
    model: nomic-embed-text:latest
    roles:
      - embed
  - name: Autodetect
    provider: ollama
    model: AUTODETECT

# Adicione estas linhas:
contextProviders:
  - name: react-docs
    params:
      url: http://localhost:8000/api/search/
      title: React Docs
      description: Busca semÃ¢ntica na documentaÃ§Ã£o do React
```

**Salve o arquivo.**

### 3. Como usar no chat do Continue

- No VS Code, abra o painel do Continue (Ctrl+Shift+P > "Continue: Open Chat").
- No chat, digite algo como:
  ```
  @react-docs como usar o hook useEffect?
  ```
- O Continue vai chamar sua API, obter os trechos mais relevantes e incluÃ­-los como contexto para o modelo (que pode ser o `llama3.1:8b` ou o `qwen2.5-coder` que vocÃª configurou).

### 4. OpÃ§Ã£o alternativa: usando um script intermediÃ¡rio (caso prefira)

Se o provider HTTP nÃ£o funcionar, vocÃª pode criar um script Python que chama a API e configurÃ¡-lo como um `contextProvider` do tipo `script`.

Crie o arquivo `/home/buids/react_mcp/mcp_client.py` (ajuste o caminho para o seu sistema):

```python
#!/usr/bin/env python3
import sys
import requests
import json

query = sys.argv[1]
response = requests.post("http://localhost:8000/api/search/", json={"query": query})
data = response.json()
for r in data["results"]:
    print(f"--- {r['metadata']['source']} ---")
    print(r['document'][:500] + "...\n")
```

DÃª permissÃ£o de execuÃ§Ã£o:
```bash
chmod +x /home/buids/react_mcp/mcp_client.py
```

Depois, no `config.yaml`, substitua o bloco `contextProviders` por:

```yaml
contextProviders:
  - name: react-docs-script
    params:
      command: python
      args:
        - /home/buids/react_mcp/mcp_client.py
        - $INPUT
```

---

## ğŸ§  Como Funciona (Arquitetura)

1. **IngestÃ£o**:  
   - O script `ingest_docs.py` percorre todos os arquivos `.md`/`.mdx` da pasta fornecida.
   - Divide o conteÃºdo em chunks de ~800 tokens (com overlap de 200).
   - Gera embeddings para cada chunk usando `all-MiniLM-L6-v2`.
   - Armazena os vetores no ChromaDB (pasta `./chroma_db`), junto com metadados (caminho do arquivo).

2. **Busca**:  
   - Quando a IA faz uma pergunta, a API recebe a query, gera o embedding correspondente e consulta o ChromaDB.
   - Retorna os K chunks mais similares, com seus textos e metadados.

3. **Contexto para a IA**:  
   - O Continue insere esses trechos no prompt, permitindo que o modelo responda com informaÃ§Ãµes atualizadas e precisas da documentaÃ§Ã£o.

---

## ğŸŒ Usando com Outros Frameworks

Para trocar a documentaÃ§Ã£o (ex.: de React para Vue), basta rodar a ingestÃ£o novamente apontando para o novo repositÃ³rio. A coleÃ§Ã£o `react_docs` serÃ¡ substituÃ­da. Se quiser manter mÃºltiplas coleÃ§Ãµes simultaneamente, vocÃª pode:

- Modificar o script de ingestÃ£o para aceitar um nome de coleÃ§Ã£o como parÃ¢metro.
- Criar mÃºltiplos endpoints na API (ex.: `/api/search/react/`, `/api/search/vue/`).

Caso precise de ajuda para expandir, consulte a seÃ§Ã£o **PersonalizaÃ§Ãµes** abaixo.

---

## ğŸ”§ PersonalizaÃ§Ãµes PossÃ­veis

- **Modelo de embedding**: Troque `all-MiniLM-L6-v2` por um modelo mais especializado (ex.: `intfloat/e5-small-v2`, `BAAI/bge-small-en`). Lembre-se de ajustar o tamanho dos vetores se necessÃ¡rio.
- **Tamanho dos chunks**: Altere `chunk_size` e `chunk_overlap` no script de ingestÃ£o para melhor precisÃ£o/desempenho.
- **Suporte a outros formatos**: Adapte o script para ler `.rst`, `.html` ou cÃ³digo-fonte. O LangChain possui splitters especÃ­ficos (ex.: `RecursiveCharacterTextSplitter` para linguagens de programaÃ§Ã£o).
- **MÃºltiplas coleÃ§Ãµes**: Modifique a view para aceitar um parÃ¢metro opcional `collection` na requisiÃ§Ã£o e busque na coleÃ§Ã£o correspondente.

---

## ğŸ› ï¸ Troubleshooting

### Erro "You are using a deprecated configuration of Chroma"
**SoluÃ§Ã£o**: Use `chromadb.PersistentClient(path="./chroma_db")` em vez da sintaxe antiga. O cÃ³digo jÃ¡ estÃ¡ corrigido nos arquivos fornecidos.

### A API retorna resultados vazios ou ruins
- Verifique se a ingestÃ£o foi concluÃ­da com sucesso (veja o total de chunks).
- Teste com uma query muito especÃ­fica que existe na documentaÃ§Ã£o.
- Aumente o `k` (nÃºmero de resultados) na requisiÃ§Ã£o.

### O Continue nÃ£o encontra o provider `react-docs`
- Certifique-se de que o arquivo `config.yaml` estÃ¡ com a indentaÃ§Ã£o correta (YAML Ã© sensÃ­vel a espaÃ§os).
- Reinicie o VS Code apÃ³s salvar o arquivo.
- Verifique se o servidor Django estÃ¡ rodando (`python manage.py runserver`).

### O modelo nÃ£o usa o contexto fornecido
- Alguns modelos podem ignorar contexto muito longo. Tente reduzir o nÃºmero de chunks retornados (parÃ¢metro `k`).
- No Continue, vocÃª pode forÃ§ar o uso do contexto digitando explicitamente `@react-docs` antes da pergunta.

---

## ğŸ“„ Arquivos do Projeto (Estrutura)

```
mcp-docs-server/
â”œâ”€â”€ manage.py
â”œâ”€â”€ config/               # pasta do projeto Django
â”‚   â”œâ”€â”€ settings.py
â”‚   â”œâ”€â”€ urls.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ docs/                     # app principal
â”‚   â”œâ”€â”€ management/
â”‚   â”‚   â””â”€â”€ commands/
â”‚   â”‚       â””â”€â”€ ingest_docs.py   # script de ingestÃ£o
â”‚   â”œâ”€â”€ views.py                  # endpoint da API
â”‚   â””â”€â”€ urls.py
â”œâ”€â”€ chroma_db/                # banco vetorial (criado na ingestÃ£o)
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md                 # este arquivo
```

---

## ğŸ“Œ ConclusÃ£o

Agora vocÃª tem um **servidor de documentaÃ§Ã£o inteligente** que pode alimentar sua IA local com contexto preciso de qualquer framework. Basta clonar a documentaÃ§Ã£o desejada, rodar a ingestÃ£o e comeÃ§ar a usar no Continue.dev.

Se encontrar problemas ou quiser sugerir melhorias, sinta-se Ã  vontade para abrir uma issue ou contribuir com o projeto.